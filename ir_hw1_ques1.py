# -*- coding: utf-8 -*-
"""IR_HW1_ques1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jWn-pj9r4Qc9761GZgSzm1clWK03DO8J

# Unzip Data
"""

#unzip data

from google.colab import drive
drive.mount('/content/gdrive')
!unzip gdrive/My\ Drive/data/Humor,Hist,Media,Food.zip

"""# Preprocessing"""

FILE_PATH = 'Humor,Hist,Media,Food/'
# FILE_NAME = 'apsnet.txt'
# with open(FILE_PATH + FILE_NAME, 'r') as f:
#   data = f.read()

"""## Expanding Contractions & Unidecode"""

!pip install contractions
!pip install Unidecode

import contractions, unidecode

# data = unidecode.unidecode(contractions.fix(data))

"""## Tokenizing"""

import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')  # for tokenization

# tokenized_data = word_tokenize(data)

"""## Remove Punctuations"""

nltk.download('punkt')

# def remove_punctuations(data):
#   return [word for word in data if word.isalpha()]

# no_punctuations = remove_punctuations(tokenized_data)
# no_punctuations

"""## Remove Stopwords & Remove Whitespace"""

nltk.download('stopwords')  # for filtering stop words
from nltk.corpus import stopwords
STOP_WORDS = set(stopwords.words('english'))

# filtered_data = [word.strip() for word in no_punctuations if word not in STOP_WORDS]
# filtered_data

"""## Lemmatization & Lower Case"""

from nltk.stem.wordnet import WordNetLemmatizer
nltk.download('wordnet')

# LEMMATIZER = WordNetLemmatizer()
# lem_sent = [LEMMATIZER.lemmatize(words_sent).lower() for words_sent in filtered_data]
# print(lem_sent)

"""## Combined Steps"""

from nltk.stem import PorterStemmer
# self.tokens[i] = ps.stem(self.tokens[i]);

def remove_punctuation(data):
  symbols = "!\"#$%&()*+-./:;<=>?@[\]^_`{|}~\n"
  for i in range(len(symbols)):
      data = data.replace(symbols[i], ' ')
      data = data.replace("  ", " ")
  data = data.replace(',', ' ')
  return data



def preprocessing(data):
  LEMMATIZER = WordNetLemmatizer()
  ps = PorterStemmer()
  tokens = word_tokenize(unidecode.unidecode(contractions.fix(remove_punctuation(data))))

  preprocessed_tokens = [LEMMATIZER.lemmatize(word.strip()).lower() for word in tokens if word.isalpha() and word.lower() not in STOP_WORDS]
  # preprocessed_tokens = [ps.stem(word.strip()).lower() for word in tokens if word.isalpha() and word not in STOP_WORDS]
  return preprocessed_tokens

"""# Inverted Index Data Structure"""

def get_data(file_name):
  with open(FILE_PATH + file_name, 'r') as f:
    return f.read()

from os import listdir
FILE_LIST = listdir(FILE_PATH)
inverted_index = {}
for document_id in range(len(FILE_LIST)):
  document = FILE_LIST[document_id]
  try:
    tokens = list(set(preprocessing(get_data(document)))) # get unique tokens only
  except UnicodeDecodeError:
    data = open(FILE_PATH + document, 'rb').read().decode('ISO-8859-1')
    tokens = list(set(preprocessing(data)))
  for token in tokens:
    if token in inverted_index.keys():
      inverted_index[token][0] += 1
      inverted_index[token][1].append(document_id)
    else:
      inverted_index[token] = [1,[document_id]]

inverted_index.keys()

# inverted_index['lion']
TOTAL_DOCS = len(FILE_LIST)

def handle_OR(l1, l2):
  l3 = []

  index1 = 0
  index2 = 0
  
  len1 = len(l1)
  len2 = len(l2)

  comparisons = 0

  while index1 < len1 and index2 < len2:
    if l1[index1] < l2[index2]:
      l3.append(l1[index1])
      index1 += 1
    elif l2[index2] < l1[index1]:
      l3.append(l2[index2])
      index2 += 1
    else:
      l3.append(l1[index1])
      index1 += 1
      index2 += 1
    
    comparisons += 1
  
  if index1 < len1:
    l3.extend(l1[index1:])
  
  if index2 < len2:
    l3.extend(l2[index2:])
  
  return l3, comparisons

def handle_AND(l1, l2):
  l3 = []

  index1 = 0
  index2 = 0
  
  len1 = len(l1)
  len2 = len(l2)

  comparisons = 0

  while index1 < len1 and index2 < len2:
    if l1[index1] < l2[index2]:
      index1 += 1
    elif l2[index2] < l1[index1]:
      index2 += 1
    else:
      l3.append(l1[index1])
      index1 += 1
      index2 += 1
    
    comparisons += 1
  return l3, comparisons

def handle_NOT(l):
  not_l = []
  prev_docid = 0
  for doc_id in l:
    not_l.extend([i for i in range(prev_docid+1, doc_id)])
    prev_docid = doc_id
  
  not_l.extend([i for i in range(prev_docid+1, TOTAL_DOCS+1)])
  return not_l, len(l)

def handle_AND_NOT(l1, l2):
  # 1 0 1
  # 1 1 0
  # 0 1 0
  # 0 0 0
  not_l, not_comparisons = handle_NOT(l2)
  l, and_comparisons = handle_AND(l1, not_l)
  return l, not_comparisons + and_comparisons

def handle_OR_NOT(l1, l2):
  # 1 0 1
  # 1 1 1
  # 0 1 0
  # 0 0 1
  not_l, not_comparisons = handle_NOT(l2)
  l, or_comparisons = handle_AND(l1, not_l) 
  return l, not_comparisons + or_comparisons

def handle_query(query, input_operation, inverted_index):
  query_list = preprocessing(query)

  if len(query_list) < 2:
    print("Invalid words in query")
    return [], 0
  
  tokens = []
  operation_list = []

  operation_index = 0

  if len(input_operation) < 1:
    print("Invalid num of operators")
    return [], 0

  if query_list[0] in inverted_index.keys():
    tokens.append(query_list[0])
    if query_list[1] in inverted_index.keys():
      tokens.append(query_list[1])
      operation_list.append(input_operation[0])
      operation_index = 1
  elif query_list[1] in inverted_index.keys():
    tokens.append(query_list[1])
    operation_list.append(input_operation[0])
    operation_index = 1

  for index in range(2, len(query_list)):
    if query_list[index] in inverted_index.keys():
      tokens.append(query_list[index])
      if operation_index < len(input_operation):
        operation_list.append(input_operation[operation_index])
        operation_index += 1
    else:
      print(query_list[index], "is not present in the data structure")

  if len(tokens) < 2:
    print("Number of valid tokens are less than two")
    return [], 0

  if len(operation_list) != len(tokens)-1:
    print("Invalid words in query")
    return [], 0

  l1 = inverted_index[tokens[0]][1]
  # ASSUMPTION LEN(TOKENS) > 0

  total_comparisons = 0
  for i in range(len(operation_list)):
    operation = operation_list[i]

    operation_handler = {
        "AND": handle_AND,
        "OR": handle_OR,
        "AND NOT": handle_AND_NOT,
        "OR NOT": handle_OR_NOT
    }

    if operation not in operation_handler.keys():
      print("INVALID OPERATOR", operation)
      return [], 0
  
    l1, comparisons = operation_handler[operation](l1, inverted_index[tokens[i+1]][1])
    total_comparisons += comparisons
  return l1, total_comparisons

# string = "Strangely names"
# operations = ["OR NOT"]
# ans, comparison = handle_query(string, operations, inverted_index)

query_num = input("Enter number of queries: ")
for i in range(int(query_num)):
  string = input("Input query: ")
  operations = input("Input operation sequence: ")

  ans, comparison = handle_query(string, operations.split(", "), inverted_index)

  print("Number of documents retrieved:", len(ans))
  print("Minimum number of comparisons:", comparison)
  print("List of document names:")
  doc_names = []
  for doc_id in ans:
    doc_names.append(FILE_LIST[doc_id])
  print(doc_names)

