{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_A1_Q2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6fVXMFa9mbAs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d3b71a5-d2f0-45d7-b23b-e2517b9c39f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Importing Necessary Libraries\n",
        "from os import listdir\n",
        "\n",
        "# For preprocessing\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# import unidecode\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip  /content/drive/MyDrive/Humor,Hist,Media,Food.zip -d /content/drive/MyDrive/IR_A1\n",
        "# !pip install contractions\n",
        "!pip install unidecode"
      ],
      "metadata": {
        "id": "zc0JyPKFnhr7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f6bff4b-ce44-4b91-e196-45bb94deb26c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.3-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 23.2 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 20 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 40 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 61 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 81 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 92 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 102 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 112 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 122 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 133 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 143 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 153 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 163 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 174 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 184 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 194 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 204 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 215 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 225 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 235 kB 5.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount the drive for loading data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSnMbQODIVst",
        "outputId": "5a864b59-6cf2-488a-a10c-05e010aa9df4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the files and stop words\n",
        "FILE_PATH = \"/content/drive/MyDrive/IR_A1/Humor,Hist,Media,Food/\"\n",
        "stopWords = stopwords.words('english')"
      ],
      "metadata": {
        "id": "1MZM_bqFwQXR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(data):\n",
        "    '''\n",
        "    input-> data (text corpus/ input sentence)\n",
        "    output-> data without punctuations\n",
        "    '''\n",
        "\n",
        "    # List of punctuations to remove\n",
        "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "    \n",
        "    # Check and replace each punctuations\n",
        "    for i in range(len(symbols)):\n",
        "\n",
        "        data = data.replace(symbols[i], ' ')\n",
        "        data = data.replace(\"  \", \" \")\n",
        "    \n",
        "    # Remove commas separately\n",
        "    data = data.replace(',', ' ')\n",
        "    \n",
        "    return data\n",
        "\n",
        "def preprocessing(data):\n",
        "    '''\n",
        "    input-> data (text corpus/ input sentence)\n",
        "    output-> tokens (preprocesses data)\n",
        "    '''\n",
        "    \n",
        "    # Convert the text to lower case \n",
        "    data = data.lower() \n",
        "\n",
        "    data = remove_punctuation(data)\n",
        "\n",
        "    # Word Tokenizing\n",
        "    tokens = nltk.word_tokenize(data)\n",
        "    \n",
        "    pTokens = []\n",
        "\n",
        "    # Remove Stopwords, punctuation marks and blank tokens\n",
        "    for word in tokens:\n",
        "        # For each word check the following: extra spaces, punctuations, stop words and numbers\n",
        "        if (word.isalpha() and word not in stopWords):\n",
        "\n",
        "            # Append the correct words\n",
        "            pTokens.append(word.strip().translate(str.maketrans('', '', string.punctuation)))\n",
        "        \n",
        "    # Return the final tokens\n",
        "    return pTokens"
      ],
      "metadata": {
        "id": "nOWw6-y9ujaR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(file_name):\n",
        "    '''\n",
        "    input-> Name of file (assumption: exists in the corpus)\n",
        "    output-> The written data in the file (unprocessed)\n",
        "    '''\n",
        "\n",
        "    # Open a file\n",
        "    with open(FILE_PATH + file_name, 'r') as f:\n",
        "\n",
        "        # Return the complete data\n",
        "        return f.read()"
      ],
      "metadata": {
        "id": "jlRjfK74uhJz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def positionalIndexing():\n",
        "    '''\n",
        "    input-> None\n",
        "    output-> A dictionary of suitable size and structure containing the positional indexes of all the data\n",
        "    '''\n",
        "\n",
        "    # Get all the file names in the dataset\n",
        "    FILE_LIST = listdir(FILE_PATH)\n",
        "\n",
        "    # Initialize an empty dictionary\n",
        "    positionalIndex = {}\n",
        "    \n",
        "    # Run a loop\n",
        "    for docID in range(len(FILE_LIST)):\n",
        "\n",
        "        # Get the name of the document\n",
        "        document = FILE_LIST[docID]\n",
        "\n",
        "        # Try to read the data, and handeling exceptions\n",
        "        try:\n",
        "\n",
        "            # preprocess the data from the document \n",
        "            tokens = list(preprocessing(get_data(document)))\n",
        "\n",
        "        # exception\n",
        "        except UnicodeDecodeError:\n",
        "\n",
        "            # preprocess the data from the document \n",
        "            data = open(FILE_PATH + document, 'rb').read().decode('ISO-8859-1')\n",
        "\n",
        "            # List all the tokens\n",
        "            tokens = list(preprocessing(data))\n",
        "        \n",
        "        # For each token in the processed file\n",
        "        position = 0\n",
        "        for token in tokens:\n",
        "\n",
        "            # Check if the token is already present in the dictionary\n",
        "            if token in positionalIndex.keys():\n",
        "\n",
        "                # increment the frequency\n",
        "                positionalIndex[token][0] += 1\n",
        "            \n",
        "                # Check if the document is present in the dictionary\n",
        "                if docID in positionalIndex[token][1].keys():\n",
        "\n",
        "                    # If present, increment the in - doc frequency and add the new position to list of positions\n",
        "                    positionalIndex[token][1][docID][0] += 1\n",
        "                    positionalIndex[token][1][docID].append(position)\n",
        "\n",
        "                else:\n",
        "\n",
        "                    # If doc ID not present, add the document as a key and the postion in a new list\n",
        "                    positionalIndex[token][1][docID] = [1, position]\n",
        "                position += 1\n",
        "           \n",
        "            else:\n",
        "\n",
        "                # If the token is not present, add the token, document ID and position\n",
        "                positionalIndex[token] = [1, {docID:[1, position]}]\n",
        "\n",
        "                # Increment the position for the next token\n",
        "                position += 1\n",
        "\n",
        "\n",
        "    # Return the positional index dictionary\n",
        "    return positionalIndex\n",
        "\n",
        "\n",
        "def getDocName(docID):\n",
        "    '''\n",
        "    input-> Doc ID (index of document in FILE_LIST)\n",
        "    output-> The name of the document as in the folder.\n",
        "    '''\n",
        "\n",
        "    # Get the file list and return the name using the index\n",
        "    FILE_LIST = listdir(FILE_PATH)\n",
        "    return FILE_LIST[docID]\n",
        "\n"
      ],
      "metadata": {
        "id": "ku_7WLnaiyDg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function to make the positional indexes\n",
        "pDict = positionalIndexing()"
      ],
      "metadata": {
        "id": "SANxm0vL7mom"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocessInput(inputWord):\n",
        "    '''\n",
        "    input-> Input query\n",
        "    output-> Preprocessed query\n",
        "    '''\n",
        "\n",
        "    # Use the same preprocessing as the data\n",
        "    return preprocessing(inputWord)\n",
        "\n",
        "\n",
        "\n",
        "def getWordfrequency(preprocessedWord):\n",
        "    '''\n",
        "    input-> Preprocessed query\n",
        "    output-> True if the word is present in dictionary, False otherwise\n",
        "    '''\n",
        "    \n",
        "    # Check if the word is present in the pDict\n",
        "    if preprocessedWord in pDict.keys():\n",
        "\n",
        "        # Get the frequency\n",
        "        freq = pDict[preprocessedWord][0]\n",
        "\n",
        "        # Return the flag and frequency\n",
        "        return True, freq\n",
        "\n",
        "    # Return False if the word is not present\n",
        "    return False, 0\n",
        "\n",
        "\n",
        "\n",
        "def getListNames(docIDs): \n",
        "    '''\n",
        "    input-> List/ Set of docIds\n",
        "    output-> List of name of the documents\n",
        "    '''\n",
        "   \n",
        "    docNames = []\n",
        "\n",
        "    # Check the type of input depending on the length of input query\n",
        "    if type(docIDs) == set:\n",
        "\n",
        "        # Get the name of each document\n",
        "        for docId in docIDs:\n",
        "            docNames.append(getDocName(docId))\n",
        "        \n",
        "        # Return the names\n",
        "        return docNames\n",
        "    \n",
        "    # If the input is dictionary\n",
        "    for docId in docIDs.keys():\n",
        "        docNames.append(getDocName(docId))\n",
        "    \n",
        "    return docNames\n",
        "\n",
        "    \n",
        "        \n",
        "\n",
        "def getIndexes(input):\n",
        "    '''\n",
        "    input-> Input query\n",
        "    output-> print the number of matched documents and their names. Return the completion statement of search.\n",
        "    '''\n",
        "\n",
        "    # Process the input query\n",
        "    tokens = preprocessInput(input)\n",
        "\n",
        "    # Check length of tokens\n",
        "    if (len(tokens) == 0):\n",
        "        return \"No words left (after preprocessing)\"\n",
        "    \n",
        "    # Check if all the words are present in the dictionary\n",
        "    for word in tokens:\n",
        "        \n",
        "        # Check if any token is not available in the dictionary, return\n",
        "        flag, wordFrequency = getWordfrequency(word)\n",
        "\n",
        "        if not flag:\n",
        "            return \"Word(s) unavailable\"\n",
        "\n",
        "                \n",
        "    \n",
        "    # get the frequency of initial word for output\n",
        "    flag, frequency = getWordfrequency(tokens[0])\n",
        "    \n",
        "    # Otherwise, check if the length of input is 1. Return all the documents with the token\n",
        "    if len(tokens) == 1:\n",
        "\n",
        "        # All the documents\n",
        "        docs = pDict[tokens[0]][1]\n",
        "\n",
        "        print(\"Search Complete\")\n",
        "        print(\"The number of documents retrieved :\", frequency)\n",
        "\n",
        "        # Print the names of the documents\n",
        "        print(\"The list of documents names \")\n",
        "        print(getListNames(docs))\n",
        "\n",
        "        \n",
        "    # If the input query is longer than 1.\n",
        "    else:\n",
        "\n",
        "        # Get the documents that contain the first word\n",
        "        startWord = tokens[0]\n",
        "        startDocs = pDict[startWord][1]\n",
        "        docs = []\n",
        "        \n",
        "        # For all the documents in the position list of first word\n",
        "        for doc in startDocs.keys():\n",
        "            docFreq = startDocs[doc][0]\n",
        "\n",
        "            # Get the position of first token in the document\n",
        "            docList = startDocs[doc][1:]\n",
        "\n",
        "            wordCount = 0\n",
        "\n",
        "            # For each position present\n",
        "            for position in docList:\n",
        "                posn = position\n",
        "\n",
        "                # Check if all the next tokens are present at the subsequent positions\n",
        "                for word in tokens[1:]:\n",
        "\n",
        "                    # Increment the position\n",
        "                    posn += 1\n",
        "\n",
        "                    # Check if the next token is present in the document, if present check if the token is present at correct position\n",
        "                    if doc in pDict[word][1].keys() and posn in pDict[word][1][doc]:\n",
        "\n",
        "                        # If present, add the document\n",
        "                        docs.append(doc)\n",
        "\n",
        "                    else:\n",
        "\n",
        "                        # Check the next positions\n",
        "                        break\n",
        "\n",
        "            \n",
        "        # Return the frequency and names\n",
        "        print(\"Search Complete\")\n",
        "\n",
        "        # Take the documents only once\n",
        "        print(\"The number of documents retrieved :\", len(set(docs)))\n",
        "        print(\"The list of documents names \")\n",
        "        print(getListNames(set(docs)))\n",
        "\n",
        "\n",
        "    # Search complete\n",
        "    return \"Program Terminated\""
      ],
      "metadata": {
        "id": "efxgV36VNuc8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--Phrase Query Search--\\n\")\n",
        "\n",
        "# Enter the input\n",
        "inputSearch = input('Enter the Phrase\\n')\n",
        "print()\n",
        "# inputSearch = 'first aid'\n",
        "\n",
        "# Initiate the search for the input query\n",
        "getIndexes(inputSearch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "oVOwPCKvm0Cx",
        "outputId": "9eb23139-9420-4865-ce5d-049c1b1fff2a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--Phrase Query Search--\n",
            "\n",
            "Enter the Phrase\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'No words left (after preprocessing)'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}